{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        \"\"\"the decision tree is a dictionary, the key is the value \n",
    "        of the feature, the value is the subtree\"\"\"\n",
    "        self.tree = {}\n",
    "\n",
    "    def entropy(self, x):  # x can be a 1d array\n",
    "        \"\"\"return H(X)\"\"\"\n",
    "        rv, counts = np.unique(x, return_counts=True)\n",
    "        pdf = counts / len(x)\n",
    "        entropy = 0\n",
    "        for prob in pdf:\n",
    "            # if the probability is 0, the entropy is 0 according to Lopital's rule\n",
    "            if prob > 0:\n",
    "                entropy -= prob * np.log2(prob)\n",
    "        return entropy\n",
    "\n",
    "    def info_gain(self, x, y):\n",
    "        \"\"\"return H(Y) - H(Y|X)\"\"\"\n",
    "        H_Y = self.entropy(y)\n",
    "        entropy_current = []\n",
    "        x_rv, counts = np.unique(x, return_counts=True)\n",
    "        x_pdf = counts / len(x)\n",
    "        for value in x_rv:\n",
    "            H_Y_X = self.entropy(y[x == value])\n",
    "            entropy_current.append(H_Y_X)\n",
    "        H_Y_X = np.sum((np.array(entropy_current)) * x_pdf)\n",
    "        return H_Y - H_Y_X\n",
    "\n",
    "    def argmax_ig(self, features_data, lables):\n",
    "        \"\"\"find the index of the feature that has the largest information gain\"\"\"\n",
    "        ig_max = 0\n",
    "        feature_index = None\n",
    "        n_features = len(features_data[0])\n",
    "        for i in range(n_features):\n",
    "            ig_current = self.info_gain(features_data[:, i], lables)\n",
    "            if ig_current > ig_max:\n",
    "                ig_max = ig_current\n",
    "                feature_index = i\n",
    "        return feature_index\n",
    "\n",
    "    def train(self, features, lables):\n",
    "        \"\"\"build a decision tree(the dictionary) recursively and return it\"\"\"\n",
    "        ig_max_id = self.argmax_ig(features, lables)\n",
    "        if ig_max_id is None: # no information gain  \n",
    "            # return the most frequent class\n",
    "            return np.argmax(np.bincount(lables))\n",
    "        else:# there is still information gain\n",
    "            tree = {}\n",
    "            rv_selected_feature = np.unique(features[:, ig_max_id])\n",
    "            for x_i in rv_selected_feature:\n",
    "                # find the samples that have the value of the feature\n",
    "                index = np.where(features[:, ig_max_id] == x_i)\n",
    "                features_sub = features[index]\n",
    "                lables_sub = lables[index]\n",
    "                # delete the feature column\n",
    "                features_sub = np.delete(features_sub, ig_max_id, axis=1)\n",
    "                tree[x_i] = self.train(features_sub, lables_sub)\n",
    "        return tree\n",
    "\n",
    "    def fit(self, features, lables):\n",
    "        self.tree = self.train(features, lables)\n",
    "\n",
    "    def predict(self, tree, sample):\n",
    "        \"\"\"predict the class of the sample\"\"\"\n",
    "        if isinstance(tree, dict):\n",
    "            for key, value in tree.items():\n",
    "                if key == sample[0]:\n",
    "                    return self.predict(value, sample[1:])\n",
    "        else:\n",
    "            return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = np.loadtxt('lenses.data',dtype='str')\n",
    "tree = DecisionTree()\n",
    "features = original_data[:,1:-1].astype('int')\n",
    "feature_names = ['feature1', 'feature2', 'feature3', 'feature4']\n",
    "lables = original_data[:,-1].astype('int')\n",
    "tree.fit(features, lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if feature2 = 1:\n",
      "\tclass --> 3\n",
      "if feature3 = 2:\n",
      "\tif feature2 = 1:\n",
      "\t\tif feature2 = 1:\n",
      "\t\t\tclass --> 2\n",
      "\t\tif feature3 = 2:\n",
      "\t\t\tclass --> 2\n",
      "\t\tif feature4 = 3:\n",
      "\t\t\tif feature2 = 1:\n",
      "\t\t\t\tclass --> 3\n",
      "\t\t\tif feature3 = 2:\n",
      "\t\t\t\tclass --> 2\n",
      "\tif feature3 = 2:\n",
      "\t\tif feature2 = 1:\n",
      "\t\t\tclass --> 1\n",
      "\t\tif feature3 = 2:\n",
      "\t\t\tif feature2 = 1:\n",
      "\t\t\t\tclass --> 1\n",
      "\t\t\tif feature3 = 2:\n",
      "\t\t\t\tclass --> 3\n",
      "\t\t\tif feature4 = 3:\n",
      "\t\t\t\tclass --> 3\n"
     ]
    }
   ],
   "source": [
    "def show_tree(tree, depth=0):\n",
    "    \"\"\"print the tree\"\"\"\n",
    "    if isinstance(tree, dict):# it is a subtree\n",
    "        for key, value in tree.items():\n",
    "            print('\\t' * depth + 'if '+ str(feature_names[key]) + ' = ' + str(key)+ ':')\n",
    "            show_tree(value, depth + 1)\n",
    "    else:#it is a leaf\n",
    "        print('\\t' * depth + 'class --> ' + str(tree))\n",
    "\n",
    "show_tree(tree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
